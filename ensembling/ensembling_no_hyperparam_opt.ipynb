{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining different machine learning algorithms into a model ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model ensembling** is a class of techniques for aggregating together multiple different predictive algorithms into a sort of mega-algorithm, which can often increase the accuracy and reduce the overfitting of your model. Ensembling approaches often work surprisingly well. Many winners of competitive data science competitions use model ensembling in [one](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf) [form](https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/5283/winning-solution-code-and-methodology) [or](http://arxiv.org/pdf/0911.0460.pdf) another. In a previous tutorial, we discussed how optimizing hyperparameters allows you to get the best performance from individual machine learning algorithms. Here, we will take you through the steps of building your own ensemble for a classification problem, consisting of an individually optimized:\n",
    "1. Random forest\n",
    "2. Support vector machine and\n",
    "3. Neural network\n",
    "\n",
    "These algorithms have quite different structures, which suggests they might capture different aspects of the dataset and could work well in an ensemble. We’ll be working on the famous `spam` dataset and trying to predict whether a certain email is spam or not, and using the standard Python machine learning stack (`scikit`/`numpy`/`pandas`).\n",
    "\n",
    "\n",
    "## The motivation behind ensembling\n",
    "The general idea behind ensembling is this: different classes of algorithms (or differently parameterized versions of the same type of algorithm) might be good at picking up on different signals in the dataset.  Combining them means that you can model the data better, leading to better predictions. Furthermore, different algorithms might be overfitting to the data in various ways, but by combining them, you can effectively average away some of this overfitting. Furthermore, if you're trying to improve your model to chase accuracy points, ensembling is a more computationally effective way to do this than trying to tune a single model by searching for more and more optimal hyperparameters.\n",
    "\n",
    "There are also fundamental reasons for why ensembling together different algorithms often improves accuracy. \n",
    "\n",
    "It is best to ensemble together models which are less correlated, because then you can capture different aspects of the blog post (see an excellent explanation [here](http://mlwave.com/kaggle-ensembling-guide/)). \n",
    "See an excellent explanation of ensembling [here](http://mlwave.com/kaggle-ensembling-guide/). \n",
    "\n",
    "## Examples of ensemble learning\n",
    "You have probably already encountered several uses of model ensembling. **Random forests** are a type of ensemble algorithm that aggregates together many individual classification tree **base learners**. They are a good systems for intuitively understanding what ensembling is. [Explanation here]. A random forest is already an ensemble. But, a random forest will be just one model in the ensemble we build here. 'Ensembling' is a broad term, and is a recurrent concept throughout machine learning. Correcting individual parts that may go wrong, so the overall thing is correct. \n",
    "\n",
    "If you’re interested in **deep learning**, one common technique for improving classification accuracy is training different networks and getting them to vote on classifications for test instances (look at **dropout** for a related but wacky take on ensembling subnetworks). Combinging different models is a recurring trend in machine learning, different incarnations. If you’re familiar with **bagging** or **boosting** algorithms, these are very explicit examples of ensembling. \n",
    "\n",
    "\n",
    "## In this post\n",
    "\n",
    "We will be working on ensembling different algorithms, using both majority voting and stacking,, in order to get improved classification accuracy on the spam dataset. We won’t do fancy visualizations of the dataset, but check out a previous tutorial or our bootcamp to learn Plotly and matplotlib if you're interested. Here, we focus on combining different algorithms to boost performance.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading up the data\n",
    "\n",
    "Load dataset. We often want our input data to be a matrix (X) and the vector of instance labels as a separate vector (y). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>is_spam</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_415</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_random</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>char_freq_(</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3628</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.824</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1540</td>\n",
       "      <td>no</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2.176</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4460</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1.023</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2771</td>\n",
       "      <td>no</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_id is_spam  word_freq_will  word_freq_original  word_freq_415  \\\n",
       "0      3628      no            0.00                   0              0   \n",
       "1        63      no            0.00                   0              0   \n",
       "2      1540      no            1.31                   0              0   \n",
       "3      4460     yes            0.75                   0              0   \n",
       "4      2771      no            0.00                   0              0   \n",
       "\n",
       "   word_freq_mail  char_freq_#  char_freq_$  word_freq_internet  \\\n",
       "0            0.00            0            0                 0.0   \n",
       "1            0.49            0            0                 0.0   \n",
       "2            0.00            0            0                 0.0   \n",
       "3            0.50            0            0                 0.5   \n",
       "4            0.00            0            0                 0.0   \n",
       "\n",
       "   word_freq_edu     ...       word_freq_receive  word_freq_000  \\\n",
       "0              0     ...                    0.00              0   \n",
       "1              0     ...                    0.00              0   \n",
       "2              0     ...                    0.00              0   \n",
       "3              0     ...                    0.25              0   \n",
       "4              0     ...                    0.00              0   \n",
       "\n",
       "   capital_run_length_average  word_freq_address  word_freq_george  \\\n",
       "0                       2.000               0.00              0.00   \n",
       "1                       2.824               0.00              0.99   \n",
       "2                       2.176               0.00              0.00   \n",
       "3                       1.023               0.75              0.00   \n",
       "4                       1.500               0.00              1.56   \n",
       "\n",
       "   word_freq_cs  word_freq_random  word_freq_conference  word_freq_technology  \\\n",
       "0             0                 0                     0                     0   \n",
       "1             0                 0                     0                     0   \n",
       "2             0                 0                     0                     0   \n",
       "3             0                 0                     0                     0   \n",
       "4             0                 0                     0                     0   \n",
       "\n",
       "   char_freq_(  \n",
       "0        0.000  \n",
       "1        0.062  \n",
       "2        0.431  \n",
       "3        0.180  \n",
       "4        0.180  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wget\n",
    "\n",
    "# Import the dataset\n",
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "columns_url = \n",
    "dataset = wget.download(data_url)\n",
    "columns = wget.download(columns_url)\n",
    "dataset = pd.read_csv(dataset, sep=\",\", )\n",
    "\n",
    "# Take a peak at the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning up and summarizing the data\n",
    "Lookin' good! Let's convert the data into a nice format. We rearrange some columns, check out what the columns are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 62)\n",
      "['word_freq_will' 'word_freq_original' 'word_freq_415' 'word_freq_mail'\n",
      " 'char_freq_#' 'char_freq_$' 'word_freq_internet' 'word_freq_edu'\n",
      " 'word_freq_hp' 'word_freq_lab']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_original</th>\n",
       "      <th>word_freq_415</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_edu</th>\n",
       "      <th>word_freq_hp</th>\n",
       "      <th>word_freq_lab</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>word_freq_random</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>char_freq_(</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.537950</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>0.189840</td>\n",
       "      <td>0.022792</td>\n",
       "      <td>0.066014</td>\n",
       "      <td>0.073210</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.611970</td>\n",
       "      <td>0.118610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051040</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>4.857610</td>\n",
       "      <td>0.149980</td>\n",
       "      <td>0.775740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036690</td>\n",
       "      <td>0.125580</td>\n",
       "      <td>0.144783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.831747</td>\n",
       "      <td>0.173041</td>\n",
       "      <td>0.365678</td>\n",
       "      <td>0.496022</td>\n",
       "      <td>0.109007</td>\n",
       "      <td>0.248239</td>\n",
       "      <td>0.270431</td>\n",
       "      <td>0.86285</td>\n",
       "      <td>1.734907</td>\n",
       "      <td>0.746169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192314</td>\n",
       "      <td>0.358906</td>\n",
       "      <td>30.226395</td>\n",
       "      <td>0.955315</td>\n",
       "      <td>3.509211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.268434</td>\n",
       "      <td>0.449092</td>\n",
       "      <td>0.232423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.541000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.219500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.396500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.250000</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>4.017000</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>20.830000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>33.330000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>2.941000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_will  word_freq_original  word_freq_415  word_freq_mail  \\\n",
       "count     1000.000000         1000.000000    1000.000000     1000.000000   \n",
       "mean         0.537950            0.038370       0.054690        0.189840   \n",
       "std          0.831747            0.173041       0.365678        0.496022   \n",
       "min          0.000000            0.000000       0.000000        0.000000   \n",
       "25%          0.000000            0.000000       0.000000        0.000000   \n",
       "50%          0.000000            0.000000       0.000000        0.000000   \n",
       "75%          0.820000            0.000000       0.000000        0.000000   \n",
       "max          6.250000            2.220000       4.760000        5.260000   \n",
       "\n",
       "       char_freq_#  char_freq_$  word_freq_internet  word_freq_edu  \\\n",
       "count  1000.000000  1000.000000         1000.000000     1000.00000   \n",
       "mean      0.022792     0.066014            0.073210        0.18100   \n",
       "std       0.109007     0.248239            0.270431        0.86285   \n",
       "min       0.000000     0.000000            0.000000        0.00000   \n",
       "25%       0.000000     0.000000            0.000000        0.00000   \n",
       "50%       0.000000     0.000000            0.000000        0.00000   \n",
       "75%       0.000000     0.016000            0.000000        0.00000   \n",
       "max       1.410000     4.017000            3.570000       10.00000   \n",
       "\n",
       "       word_freq_hp  word_freq_lab     ...       word_freq_receive  \\\n",
       "count   1000.000000    1000.000000     ...             1000.000000   \n",
       "mean       0.611970       0.118610     ...                0.051040   \n",
       "std        1.734907       0.746169     ...                0.192314   \n",
       "min        0.000000       0.000000     ...                0.000000   \n",
       "25%        0.000000       0.000000     ...                0.000000   \n",
       "50%        0.000000       0.000000     ...                0.000000   \n",
       "75%        0.315000       0.000000     ...                0.000000   \n",
       "max       20.830000      14.280000     ...                2.000000   \n",
       "\n",
       "       word_freq_000  capital_run_length_average  word_freq_address  \\\n",
       "count    1000.000000                 1000.000000        1000.000000   \n",
       "mean        0.081300                    4.857610           0.149980   \n",
       "std         0.358906                   30.226395           0.955315   \n",
       "min         0.000000                    1.000000           0.000000   \n",
       "25%         0.000000                    1.541000           0.000000   \n",
       "50%         0.000000                    2.219500           0.000000   \n",
       "75%         0.000000                    3.396500           0.000000   \n",
       "max         5.450000                  667.000000          14.280000   \n",
       "\n",
       "       word_freq_george  word_freq_cs  word_freq_random  word_freq_conference  \\\n",
       "count       1000.000000          1000              1000           1000.000000   \n",
       "mean           0.775740             0                 0              0.036690   \n",
       "std            3.509211             0                 0              0.268434   \n",
       "min            0.000000             0                 0              0.000000   \n",
       "25%            0.000000             0                 0              0.000000   \n",
       "50%            0.000000             0                 0              0.000000   \n",
       "75%            0.000000             0                 0              0.000000   \n",
       "max           33.330000             0                 0              5.000000   \n",
       "\n",
       "       word_freq_technology  char_freq_(  \n",
       "count           1000.000000  1000.000000  \n",
       "mean               0.125580     0.144783  \n",
       "std                0.449092     0.232423  \n",
       "min                0.000000     0.000000  \n",
       "25%                0.000000     0.000000  \n",
       "50%                0.000000     0.072000  \n",
       "75%                0.000000     0.195000  \n",
       "max                4.760000     2.941000  \n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder the data columns and drop email_id\n",
    "cols = dataset.columns.tolist()\n",
    "cols = cols[2:] + [cols[1]]\n",
    "dataset = dataset[cols]\n",
    "\n",
    "# Examine shape of dataset and some column names\n",
    "print dataset.shape\n",
    "print dataset.columns.values[0:10]\n",
    "\n",
    "# Summarise feature values\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert dataframe to numpy array and split\n",
    "# data into input matrix X and class label vector y\n",
    "npArray = np.array(dataset)\n",
    "X = npArray[:,:-1].astype(float)\n",
    "y = npArray[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Splitting data into training and testing sets\n",
    "\n",
    "Our day is now nice and squeaky clean! This definitely always happens in real life. \n",
    "\n",
    "Next up, let's scale the data and split it into a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Scale and split dataset\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X_scaled, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running algorithms on the data\n",
    "\n",
    "Blah blah now it's time to train algorithms. We are doing binary classification. Could ahve also used logistic regression, kNN, etc etc.\n",
    "\n",
    "### 4.1 Random forests\n",
    "\n",
    "Let’s build a random forest. A great explanation of random forests can be found here. Briefly, random forests build a collection of classification trees, which each try to predict classes by recursively splitting the data on features that split classes best. Each tree is trained on bootstrapped data, and each split is only allowed to use certain variables. So, an element of randomness is introduced, a variety of different trees are built, and the 'random forest' ensembles together these base learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.94      0.96      0.95       170\n",
      "        yes       0.91      0.86      0.88        80\n",
      "\n",
      "avg / total       0.93      0.93      0.93       250\n",
      "\n",
      "('Overall Accuracy:', 0.93)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Build a random forest using previously-optimized hyperparameter values\n",
    "clfRDF = RandomForestClassifier(n_estimators=best_n_estim, max_features=best_max_features, max_depth=best_max_depth)\n",
    "clfRDF.fit(XTrain, yTrain)\n",
    "RF_predictions = clfRDF.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, RF_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, RF_predictions),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93-95% accuracy, not too shabby! Have a look and see how random forests with suboptimal hyperparameters fare. We got around 91-92% accuracy on the out of the box (untuned) random forests, which actually isn't terrible. \n",
    "\n",
    "### 2) Second algorithm: support vector machines\n",
    "\n",
    "Let's train our second algorithm, support vector machines (SVMs) to do the same exact prediction task. A great introduction to the theory behind SVMs can be read [here](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners). Briefly, SVMs search for hyperplanes in the feature space which best divide the different classes in your dataset. Crucially, SVMs can find non-linear decision boundaries between classes using a process called kernelling, which projects the data into a higher-dimensional space. This sounds a bit abstract, but if you've ever fit a linear regression to power-transformed variables (e.g. maybe you used x^2, x^3 as features), you're already familiar with the concept.\n",
    "\n",
    "SVMs can use different types of kernels, like Gaussian or radial ones, to throw the data into a different space. The main hyperparameters we must tune for SVMs are gamma (a kernel parameter, controlling how far we 'throw' the data into the new feature space) and C (which controls the [bias-variance tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html) of the model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.94      0.95      0.94       170\n",
      "        yes       0.88      0.86      0.87        80\n",
      "\n",
      "avg / total       0.92      0.92      0.92       250\n",
      "\n",
      "Overall Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "g_range = 2. ** np.arange(-15, 5, step=2)\n",
    "C_range = 2. ** np.arange(-5, 15, step=2)\n",
    "\n",
    "hyperparameters = [{'gamma': g_range, \n",
    "                    'C': C_range}] \n",
    "\n",
    "# Grid search using cross-validation\n",
    "grid = GridSearchCV(SVC(), param_grid=hyperparameters, cv= 10)  \n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "bestG = grid.best_params_['gamma']\n",
    "bestC = grid.best_params_['C']\n",
    "\n",
    "# Train SVM and output predictions\n",
    "rbfSVM = SVC(kernel='rbf', C=bestC, gamma=bestG)\n",
    "rbfSVM.fit(XTrain, yTrain)\n",
    "SVM_predictions = rbfSVM.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, SVM_predictions)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, SVM_predictions),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! This is similar performance to what we saw in the random forests.\n",
    "\n",
    "### 3) Third algorithm: neural network\n",
    "\n",
    "Finally, let's jump on the hype wagon and throw neural networks at our problem.\n",
    "\n",
    "Neural networks (NNs) represent a different way of thinking about machine learning algorithms. A great place to start learning about neural networks and deep learning is [this resource](http://neuralnetworksanddeeplearning.com/about.html). Briefly, NNs are composed of  multiple layers of artificial neurons, which individually are simple processing units that weigh up input data. Together, layers of neurons can work together to compute some very complex functions of the data, which in turn can make excellent predictions. You may be aware of some of the crazy results that NN research has recently achieved.\n",
    "\n",
    "Here, we train a shallow, fully-connected, feedforward neural network on the spam dataset. Other types of neural network implementations in scikit are available here. The hyperparameters we optimize here are the overall architecture (number of neurons in each layer and the number of layers) and the learning rate (which controls how quickly the parameters in our network change during the training phase; see [gradient descent](http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent) and [backpropagation](http://neuralnetworksanddeeplearning.com/chap2.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         no       0.95      0.92      0.93       170\n",
      "        yes       0.84      0.89      0.86        80\n",
      "\n",
      "avg / total       0.91      0.91      0.91       250\n",
      "\n",
      "Overall Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "from multilayer_perceptron import multilayer_perceptron\n",
    "\n",
    "# Search for good hyperparameter values\n",
    "# Specify values to grid search over\n",
    "layer_size_range = [(3,2),(10,10),(2,2,2),10,5] # different networks shapes\n",
    "learning_rate_range = np.linspace(.1,1,3)\n",
    "hyperparameters = [{'hidden_layer_sizes': layer_size_range, 'learning_rate_init': learning_rate_range}]\n",
    "\n",
    "# Grid search using cross-validation\n",
    "grid = GridSearchCV(multilayer_perceptron.MultilayerPerceptronClassifier(), param_grid=hyperparameters, cv=10)\n",
    "grid.fit(XTrain, yTrain)\n",
    "\n",
    "# Output best hyperparameter values\n",
    "best_size    = grid.best_params_['hidden_layer_sizes']\n",
    "best_best_lr = grid.best_params_['learning_rate_init']\n",
    "\n",
    "# Train neural network and output predictions\n",
    "nnet = multilayer_perceptron.MultilayerPerceptronClassifier(hidden_layer_sizes=best_size, learning_rate_init=best_best_lr)\n",
    "nnet.fit(XTrain, yTrain)\n",
    "NN_predictions = nnet.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, NN_predictions)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, NN_predictions),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this neural network (given this dataset, architecture, and hyperparameterisation) is doing slightly worse on the spam dataset. That's okay, it could still be picking up on a signal that the random forest and SVM weren't. \n",
    "\n",
    "Machine learning algorithns... ensemble!\n",
    "\n",
    "### 4) Majority vote on classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-21b39eda8376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# stick all predictions into a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRF_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVM_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNN_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'RF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SVM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m predictions = pd.DataFrame(np.where(predictions=='yes', 1, 0), \n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# here's a rough solution\n",
    "import collections\n",
    "\n",
    "# stick all predictions into a dataframe\n",
    "predictions = pd.DataFrame(np.array([RF_predictions, SVM_predictions, NN_predictions])).T\n",
    "predictions.columns = ['RF', 'SVM', 'NN']\n",
    "predictions = pd.DataFrame(np.where(predictions=='yes', 1, 0), \n",
    "                           columns=predictions.columns, \n",
    "                           index=predictions.index)\n",
    "\n",
    "# initialise empty array for holding predictions\n",
    "ensembled_predictions = np.zeros(shape=yTest.shape)\n",
    "\n",
    "# majority vote and output final predictions\n",
    "for test_point in range(predictions.shape[0]):\n",
    "    predictions.iloc[test_point,:]\n",
    "    counts = collections.Counter(predictions.iloc[test_point,:])\n",
    "    majority_vote = counts.most_common(1)[0][0]\n",
    "    \n",
    "    # output votes\n",
    "    ensembled_predictions[test_point] = majority_vote.astype(int)\n",
    "    #print \"The majority vote for test point\", test_point, \"is: \", majority_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.96      0.95       170\n",
      "          1       0.91      0.89      0.90        80\n",
      "\n",
      "avg / total       0.94      0.94      0.94       250\n",
      "\n",
      "Ensemble Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Get final accuracy of ensembled model\n",
    "yTest[yTest == \"yes\"] = 1\n",
    "yTest[yTest == \"no\"] = 0\n",
    "\n",
    "print metrics.classification_report(yTest.astype(int), ensembled_predictions.astype(int))\n",
    "print \"Ensemble Accuracy:\", round(metrics.accuracy_score(yTest.astype(int), ensembled_predictions.astype(int)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Conclusion\n",
    "\n",
    "There are plenty of ways to do model ensembling. Simple majority voting. We can also do weighted majority voting, where models with higher accuracy get more of a vote. If your output is numerical, you could average. These relatively simple techniques do a great job, but there is more! Stacking (also called blending) is when the predictions from different algorithms are used as input into another algorithm (often good old linear and logistic regression) which then outputs your final predictions. For example, you might train a linear model on the predictions. Blending. \n",
    "\n",
    "\n",
    "\n",
    " It is best to ensemble together models which are less correlated (see an excellent explanation here). \n",
    "See an excellent explanation of ensembling here. \n",
    "\n",
    "\n",
    "What happens when your dataset isn’t as nice as this? What if there are many more instances of one class versus the other, or if you have a lot of missing values, or a mixture of categorical and numerical variables? Stay tuned for the next blog post where we write up guidance on tackling these types of sticky situations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "+ Should we use something cooler like gradient boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Another nice tutorial on doing ensembling in python is [here](http://sebastianraschka.com/Articles/2014_ensemble_classifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
