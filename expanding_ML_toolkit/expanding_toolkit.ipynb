{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding your machine learning toolkit: Randomized search, computational budgets, and new algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Previously, we wrote about some [common trade-offs](http://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/) in machine learning and the importance of [tuning models](http://blog.cambridgecoding.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/) to your specific dataset. We demonstrated tuning a **random forest** classifier using grid search, and how **cross-validation** can help avoid **overfitting** when tuning **hyperparameters (HPs)**.\n",
    "\n",
    "In this follow-up post, you'll beef up your machine learning toolbox by trying out some new, broadly-applicable tools. You'll learn a different strategy for traversing hyperparameter space - **randomized search** - and how to use it to tune two other classification algorithms - a **support vector machine** and a **logistic regression classifier**. \n",
    "\n",
    "![Learning algorithms we'll use in this tutorial.](https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/expanding_ML_toolkit/hyperparam_algos_logistic.png)\n",
    "\n",
    "We'll keep working with the [wine dataset](https://archive.ics.uci.edu/ml/datasets/Wine), which contains chemical characteristics of wines of varying quality. As before, our goal is to try to predict a wine's quality form these features.\n",
    "\n",
    "Here are the things we'll cover in this blog post:\n",
    "\n",
    "![Tutorial overview.](https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/expanding_ML_toolkit/hyperparam_intro_logistic.png)\n",
    "\n",
    "In the next blog post, you will learn how to take these three different tuned machine learning algorithms and combine them to build an aggregate **model ensemble**. Building ensembles often leads to improved model performance and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and train/test splitting the dataset\n",
    "\n",
    "You start off by collecting the dataset. We have covered the data loading, preprocessing,  and train/test splitting [previously](http://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/), so we won't repeat ourselves here. Also check out [this post](http://blog.cambridgecoding.com/2016/02/07/eda-and-interactive-figures-with-plotly/) on using plotly to create exploratory, interactive graphics of the wine dataset features. \n",
    "\n",
    "You can fetch and format the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Import the dataset\n",
    "data_url = 'https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/datasets/wine/winequality-red.csv'\n",
    "dataset = wget.download(data_url)\n",
    "dataset = pd.read_csv(dataset, sep=\";\")\n",
    "\n",
    "# Using a lambda function to bin quality scores\n",
    "dataset['quality_is_high'] = dataset.quality.apply(lambda x: 1 if x >= 6 else 0)\n",
    "\n",
    "# Convert the dataframe to a numpy array and split the\n",
    "# data into an input matrix X and class label vector y\n",
    "npArray = np.array(dataset)\n",
    "X = npArray[:,:-2].astype(float)\n",
    "y = npArray[:,-1]\n",
    "\n",
    "# Split into training and test sets\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing randomized search: comparison with grid search\n",
    "\n",
    "You have already built a random forest classifier, tuned using **grid search**, to predict wine quality ([here](http://localhost:8888/notebooks/polished_prediction/scanning_hyperspace.ipynb#)). Grid search is quite commonly used, and is essentially just a method that exhaustively tries out all combinations of manually prespecified HP values and reports the best option (i.e. the one leading to the highest test accuracy). The benefit of this approach is that you thoroughly test out various combinations, but this is of course very computationally expensive. For grid search to be tractable, you often have to restrict the number of combinations, which can severely limit how well you explore hyperparameter space and lead to you overlooking regions where accuracy would be highest. \n",
    "\n",
    "Another way to search through hyperparameter space to find optima is via **randomized search**. In randomized search, you sample HP values a certain number of times from some distribution which you prespecify in advance. So unlike grid search, in which you specify specific numbers to combinatorially try out, you are specifying distributions that cover the HP space you want to explore. For example, you might specify a standard normal distribution over an HP if you think reasonable values are roughly centered around 0, or a uniform distribution over some range if you think values within that range are about as likely to be \"good\". In randomized search, you also specify a `n_iter` parameter, which acts as a **computational budget**, controlling how many different parameter settings are tried out in total. \n",
    "\n",
    "We can visually summarize the grid search (grey boxes) and randomized search (purple boxes) strategies like so: \n",
    "\n",
    "![Grid search and randomized search](https://raw.githubusercontent.com/nslatysheva/data_science_blogging/master/expanding_ML_toolkit/expanding_toolkit.jpg)\n",
    "\n",
    "Here, both approaches are constrained by the same computational budget - they can only try out 9 different HP settings (i.e. certain values for HP1 and HP2). Randomized search tries out HP values from two normal distributions (purple bell curves), repeating the process 9 times and thus getting 9 different values of both HP1 and HP2. Most values fall into the meaty portion of the normal distribution, but occasionally the tails are sampled from as well - this means you have at least some chance of trying out distant regions that could potentially strike gold (i.e. the hypothetically optimal HP space leading to high accuracy, bottom right). \n",
    "\n",
    "Meanwhile, grid search tries out 3 values each of HP1 and HP2. Of course, these values do not have to be as close to each other as we have drawn (and one could indeed hit the gold space with grid search), but the idea is that since you are constrained to trying out *all* combinations of prespecified HP values, this intrinsically limits how much of the HP space can be explored. Specifically, here randomized search space has searched a space that is 16 times bigger (we drew a 3x3 box for the grid search and a 12x12 box for the larger grid). The `n_iter` argument controlling the number of HP combinations to try out gives you access to a tradeoff between computational resources invested and the HP space you can explore. \n",
    "\n",
    "Check out this [paper](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) outlining the efficiency of randomized search compared to grid search, especially in high-dimensional HP spaces. You can imagine that if you already have 12x12=144 possible combinations of 2 HPs, adding another increases the number of possibilities to search through to 12x12x12=1728. This becomes very demanding very quickly. But intuitively, as computational resources and patience approach infinity, grid search becomes the better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using randomized search\n",
    "Scikit makes using randomized search easy with [`RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV). You can feed distributions of HPs to the `RandomizedSearchCV` object in two (fairly similar) ways:\n",
    "1. You can either define distributions over HPs, without immediately sampling from them, and pass these distributions to `RandomizedSearchCV`, which will proceed to sample `n_iter` number of times *with replacement* from the distributions to generate candidate HP combinations. \n",
    "2. You can sample from distributions immediately and pass a list of possible HP values to `RandomizedSearchCV`, and it will sample from these possible values `n_iter` number of times *without replacement*. \n",
    "\n",
    "Here is what the second approach looks like with [**random forests**](http://blog.cambridgecoding.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [79, 75, 70, 78, 72], 'max_features': [4, 8, 6, 8, 1]}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Designate distributions to sample hyperparameters from \n",
    "n_estimators = np.random.uniform(70, 80, 5).astype(int)\n",
    "max_features = np.random.normal(6, 3, 5).astype(int)\n",
    "\n",
    "# Check max_features>0 & max_features<=total number of features\n",
    "max_features[max_features <= 0] = 1\n",
    "max_features[max_features > X.shape[1]] = X.shape[1]\n",
    "\n",
    "hyperparameters = {'n_estimators': list(n_estimators),\n",
    "                   'max_features': list(max_features)}\n",
    "\n",
    "print (hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing n_estimators value is:    78\n",
      "The best performing max_features value is:     1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.85      0.81       188\n",
      "        1.0       0.85      0.79      0.82       212\n",
      "\n",
      "avg / total       0.82      0.82      0.82       400\n",
      "\n",
      "('Overall Accuracy:', 0.818)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Run randomized search\n",
    "randomCV = RandomizedSearchCV(RandomForestClassifier(), param_distributions=hyperparameters, n_iter=20)\n",
    "randomCV.fit(XTrain, yTrain)\n",
    "\n",
    "# Identify optimal hyperparameter values\n",
    "best_n_estim      = randomCV.best_params_['n_estimators']\n",
    "best_max_features = randomCV.best_params_['max_features']  \n",
    "\n",
    "print(\"The best performing n_estimators value is: {:5d}\".format(best_n_estim))\n",
    "print(\"The best performing max_features value is: {:5d}\".format(best_max_features))\n",
    "\n",
    "# Train classifier using optimal hyperparameter values\n",
    "# We could have also gotten this model out from randomCV.best_estimator_\n",
    "rf = RandomForestClassifier(n_estimators=best_n_estim,\n",
    "                            max_features=best_max_features)\n",
    "\n",
    "rf.fit(XTrain, yTrain)\n",
    "rf_predictions = rf.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, rf_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, rf_predictions), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this performance to the default random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method RandomForestClassifier.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.74      0.83      0.78       188\n",
      "        1.0       0.83      0.74      0.78       212\n",
      "\n",
      "avg / total       0.79      0.78      0.78       400\n",
      "\n",
      "('Overall Accuracy:', 0.782)\n"
     ]
    }
   ],
   "source": [
    "# Create default rf\n",
    "rf = RandomForestClassifier()\n",
    "print(rf.get_params)\n",
    "\n",
    "# Fit and predict with default rf\n",
    "rf.fit(XTrain, yTrain)\n",
    "rf_predictions = rf.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, rf_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, rf_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the default performance is lower, which is generally what you might expect. Either grid search or randomized search are [good options](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html) for tuning random forests.\n",
    "\n",
    "Let's look at how to tune the two other predictors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning a support vector machine\n",
    "\n",
    "Let's train the second algorithm, a **support vector machine** (SVM) classifier, to do the same wine quality prediction task. A great introduction to the theory behind SVMs can be found in Chapter 9 of the [Introduction to Statistical Learning book](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) or [in this nice blog post](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners). Briefly, SVMs search for **separating hyperplanes** in the feature space which best divide the different classes in your dataset. If you had 2 features, SVMs would search for the best dividing line; if you had 3 features, SVMs search for the best dividing 2d plane, etc. Crucially, SVMs can construct complex, **non-linear decision boundaries** between classes by making use of a process called **kernelling**, which projects the data into a higher-dimensional space and facilitates the identification of a good boundary.\n",
    "\n",
    "SVMs can use different types of kernel functions, like linear, polynomial, Gaussian or radial kernels, to throw the data into a different space. Let's use the popular **radial basis kernel**. Of the available [hyperparameters](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), some key ones to tune are:\n",
    "\n",
    "+ `gamma` (a kernel parameter)\n",
    "+ `C` (which controls the 'softness' of the classification boundary margin and hence the [bias-variance tradeoff](http://blog.cambridgecoding.com/2016/03/24/misleading-modelling-overfitting-cross-validation-and-the-bias-variance-trade-off/) of the model\n",
    "\n",
    "Examine the default HP settings and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default SVC parameters are: \n",
      "<bound method SVC.get_params of SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "default_SVC = svm.SVC()\n",
    "print (\"Default SVC parameters are: \\n{}\".format(default_SVC.get_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default settings, the SVM performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.75      0.71       188\n",
      "        1.0       0.75      0.67      0.71       212\n",
      "\n",
      "avg / total       0.71      0.71      0.71       400\n",
      "\n",
      "('Overall Accuracy:', 0.71)\n"
     ]
    }
   ],
   "source": [
    "# Create, fit, and test default SVM\n",
    "rbfSVM = SVC(kernel='rbf')\n",
    "rbfSVM.fit(XTrain, yTrain)\n",
    "svm_predictions = rbfSVM.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, svm_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, svm_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use randomized search to try to improve on this accuracy. First, define distributions you want to sample HP values from and create a dictionary of possible values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [0.76810065955131279, 0.98399752772900562, 1.1633530245633044, 0.90232488189744564, 0.91128942729215812], 'gamma': [0.084165263215673816, 0.094211019235189056, 0.2356776715362813, 0.29525183694492491, 0.21939751533443838]}\n"
     ]
    }
   ],
   "source": [
    "# Designate distributions to sample hyperparameters from \n",
    "g_range = np.random.uniform(0.0, 0.3, 5).astype(float)\n",
    "C_range = np.random.normal(1, 0.1, 5).astype(float)\n",
    "\n",
    "# Check that gamma>0 and C>0 \n",
    "C_range[C_range < 0] = 0.0001\n",
    "\n",
    "hyperparameters = {'gamma': list(g_range), \n",
    "                    'C': list(C_range)}\n",
    "\n",
    "print (hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass this dictionary to `param_distributions` argument of `RandomizedSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing gamma value is:  0.08\n",
      "The best performing C value is:  1.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Run randomized search\n",
    "randomCV = RandomizedSearchCV(SVC(kernel='rbf'), param_distributions=hyperparameters, n_iter=20)\n",
    "randomCV.fit(XTrain, yTrain)\n",
    "\n",
    "# Identify optimal hyperparameter values\n",
    "best_gamma  = randomCV.best_params_['gamma']\n",
    "best_C      = randomCV.best_params_['C']\n",
    "\n",
    "print(\"The best performing gamma value is: {:5.2f}\".format(best_gamma))\n",
    "print(\"The best performing C value is: {:5.2f}\".format(best_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the scores of e.g. the first 5 tested HP combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.67056, std: 0.00993, params: {'C': 0.98399752772900562, 'gamma': 0.21939751533443838}, mean: 0.67556, std: 0.01047, params: {'C': 1.1633530245633044, 'gamma': 0.21939751533443838}, mean: 0.65805, std: 0.01058, params: {'C': 0.98399752772900562, 'gamma': 0.29525183694492491}, mean: 0.66222, std: 0.01571, params: {'C': 1.1633530245633044, 'gamma': 0.29525183694492491}, mean: 0.68057, std: 0.02856, params: {'C': 1.1633530245633044, 'gamma': 0.084165263215673816}]\n"
     ]
    }
   ],
   "source": [
    "print (randomCV.grid_scores_[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.75      0.71       188\n",
      "        1.0       0.75      0.67      0.71       212\n",
      "\n",
      "avg / total       0.71      0.71      0.71       400\n",
      "\n",
      "('Overall Accuracy:', 0.71)\n"
     ]
    }
   ],
   "source": [
    "# Train SVM and output predictions\n",
    "rbfSVM = SVC(kernel='rbf', C=best_C, gamma=best_gamma)\n",
    "rbfSVM.fit(XTrain, yTrain)\n",
    "svm_predictions = rbfSVM.predict(XTest)\n",
    "\n",
    "print (metrics.classification_report(yTest, svm_predictions))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(yTest, svm_predictions),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we get the same accuracy as the default model in this case. This is fine, but in general you might think about doing things like casting the net wider to try out quite different HP values, adding new HPs to the tuning process, try a different learning algorithm, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning a logistic regression classifier\n",
    "\n",
    "The final model you'll tune and apply to predict wine quality is a logistic regression classifier. This is a type of regression model which is used for predicting binary outcomes (like good wine/not good wine). Logistic regression fits a sigmoidal (S-shaped) curve through the data, but can be viewed as just a transformed version of linear regression - a straight line predicting the [log odds](https://en.wikipedia.org/wiki/Logit) of data points being in one of the two classes. A nice explanation of logistic regression can be found [here](http://online.cambridgecoding.com/notebooks/eWReNYcAfB/implementing-logistic-regression-classifier-trained-by-gradient-descent-4).\n",
    "\n",
    "One topic you will often encounter in machine learning is **regularization**, which is a class of techniques to reduce overfitting. The idea behind regularization is that you do not *only* want to maximize a model's fit to your data, since this susceptible to overfitting. Regularization techniques try to cut down on overfitting by penalizing models, for example if they use too many parameters, or if they assign coefficients or weights that are \"too big\". Regularization means that models have to learn from the data under a series of constraints, which often leads to robust representations of the data. \n",
    "\n",
    "You can adjust just how much regularization you want by adjusting **regularization hyperparameters**, and since this is something you might want to do often, scikit-learn comes with some pre-built models that can very efficiently fit data for a range of regularization hyperparameter values. This is the case for regularized linear regression models like [Lasso regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV) and [ridge regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV), which use l1 and l2 penalties, respectively, to shrink the size of the regression coefficients. These scikit modules offer a shortcut to performing cross-validated selection of the regularization hyperparameter.\n",
    "\n",
    "But you can also optimize how much regularization you want yourself, while at the same time tuning other hyperparameters (like the choice between l1 and l2 penalty), in the same manner as you've been doing.\n",
    "\n",
    "Let's examine default HP settings and performance for a logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default logistic regression parameters are: <bound method LogisticRegression.get_params of LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)>\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.74      0.73       188\n",
      "        1.0       0.77      0.75      0.76       212\n",
      "\n",
      "avg / total       0.75      0.75      0.75       400\n",
      "\n",
      "Overall Accuracy: 0.748\n"
     ]
    }
   ],
   "source": [
    "# Tuning a regularized logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Examine defaults\n",
    "default_lr = LogisticRegression()\n",
    "print (\"Default logistic regression parameters are: {}\".format(default_lr.get_params))\n",
    "       \n",
    "# Train model and output predictions\n",
    "classifier_logistic = LogisticRegression()\n",
    "classifier_logistic_fit = classifier_logistic.fit(XTrain, yTrain)\n",
    "logistic_predictions = classifier_logistic_fit.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, logistic_predictions)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, logistic_predictions),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to optimise the HPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': ['l1', 'l2'], 'C': array([ 1.02967524,  1.09405268,  1.03702083,  0.90472955,  1.18213198,\n",
      "        0.96276878,  1.02598591,  1.17511943,  1.00958543,  1.04662732])}\n"
     ]
    }
   ],
   "source": [
    "# Specify HP distributions\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "C_range = np.random.normal(1, 0.2, 10).astype(float)\n",
    "\n",
    "# Check that C>0 \n",
    "C_range[C_range < 0] = 0.0001\n",
    "\n",
    "hyperparameters = {'penalty': penalty, \n",
    "                    'C': C_range}\n",
    "\n",
    "print (hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And feeding these values into `RandomizedSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing penalty is: l1\n",
      "The best performing C value is:  0.96\n"
     ]
    }
   ],
   "source": [
    "# Randomized search using cross-validation\n",
    "randomCV = RandomizedSearchCV(LogisticRegression(), param_distributions=hyperparameters, cv=20)  \n",
    "randomCV.fit(XTrain, yTrain)\n",
    "\n",
    "best_penalty = randomCV.best_params_['penalty']\n",
    "best_C       = randomCV.best_params_['C']\n",
    "\n",
    "print (\"The best performing penalty is: {}\".format(best_penalty))\n",
    "print (\"The best performing C value is: {:5.2f}\".format(best_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these values to train a new, hopefully better model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.74      0.74       188\n",
      "        1.0       0.77      0.75      0.76       212\n",
      "\n",
      "avg / total       0.75      0.75      0.75       400\n",
      "\n",
      "Overall Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Train model and output predictions\n",
    "classifier_logistic = LogisticRegression(penalty=best_penalty, C=best_C)\n",
    "classifier_logistic_fit = classifier_logistic.fit(XTrain, yTrain)\n",
    "logistic_predictions = classifier_logistic_fit.predict(XTest)\n",
    "\n",
    "print metrics.classification_report(yTest, logistic_predictions)\n",
    "print \"Overall Accuracy:\", round(metrics.accuracy_score(yTest, logistic_predictions),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This also actually looks quite similar to the default performance. It's always important to tune hyperparameters, but the effect might vary across learning algorithms and datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have done randomized search to optimize a random forest, an SVM model, and logistic regression. Fancier techniques for hyperparameter optimization include methods based on [gradient descent](http://jmlr.org/proceedings/papers/v37/maclaurin15.pdf), grad student descent (not recommended), and [Bayesian approaches](http://arxiv.org/pdf/1206.2944.pdf) which update prior beliefs about good values of hyperparameters based on observed performance (see [Spearmint](https://github.com/JasperSnoek/spearmint) and [hyperopt](http://hyperopt.github.io/hyperopt/)). The important thing is to try out different learning algorithms and HP settings on your dataset. \n",
    "\n",
    "In the next post, we try to leverage the contribution of different types of models by building them up into an ensemble model, with the aim of increasing predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "1. Explain why the main advantages of randomized search over grid search are the following (as stated by [scikit](http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization)):\n",
    "\n",
    "    * A budget can be chosen independent of the number of parameters and possible values.\n",
    "    * Adding parameters that do not influence the performance does not decrease efficiency.      \n",
    "    \n",
    "2. `RandomizedSearchCV` will throw an error if `n_iter` is greater than the number of possible HP combinations. Why is this a useful property? What happens if the number of combinations is indeed smaller than `n_iter`, but some of the combinations happen to be duplicates?\n",
    "\n",
    "3. If the HP search space is quite small, the behaviour of randomized search can approach that of grid search. Specifically, if you were to sample values of two HPs from uniform distributions n times and set `n_iter` to $n^2$, then randomized search would be very similar to a grid search over the same HP ranges. But it wouldn't be quite identical - why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
